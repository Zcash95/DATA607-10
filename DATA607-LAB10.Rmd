---
title: "DATA607-LAB10"
author: "Biyag Dukuray"
output: html_document
date: "2024-03-28"
---

## In Text Mining with R, Chapter 2 looks at Sentiment Analysis. In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document. You should provide a citation to this base code. You’re then asked to extend the code in two ways: Work with a different corpus of your choosing, and Incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research). As usual, please submit links to both an .Rmd file posted in your GitHub repository and to your code on rpubs.com. You make work on a small team on this assignment.

```{r}
library(tidyverse)
library(dplyr)
library(tidyr)
library(tidytext)
```

```{r}
get_sentiments("afinn")
```

```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```

```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")
pride_prejudice
```

```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```

```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                    lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

```{r}
library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}
library(reshape2)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r}
p_and_p_sentences <- tibble(text = prideprejudice) %>%
  unnest_tokens(sentence, text, token = "sentences")
```

```{r}
p_and_p_sentences$sentence[2]
```

```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()
```

## I used a Corpus which contained tweets about Chatgpt. This is the description of the data. "ChatGPT has been a major talk in the tech world. The tweets about chatgpt were gathered for a month and then the sentiment analysis was made using Natural Language Processing." I will use the "Loughran" lexicon which I found in a link refrenced below to analyze the sentiment of this dataset. I will tidy this data to be able to analyze it.

```{r}
tweets_df <- read.csv("chatgpt.csv")

tweets_df$tweets <- as.character(tweets_df$tweets)

tweets_df1 <- tweets_df

tweets_list <- as.list(tweets_df1$tweets)

tweets_df1$tweets <- unlist(tweets_list)

label_counts <- tweets_df1 %>%
count(labels)

print(label_counts)
```

**I wanted to first analyze the data and see which words occur the most often in the tweets of this data frame. In order to do so I had to tidy this data to prepare it for sentiment analysis because the tweets contained characters such as emojis which cannot be assigned a sentiment.**

```{r}
tweet_words <- tweets_list %>%
unlist() %>%
tolower() %>%
str_split("\\s+") %>%
unlist()

tweet_words_df <- data.frame(word = tweet_words)

word_counts <- tweet_words_df %>%
count(word, sort = TRUE)

top_50_words <- head(word_counts, 50)

print(top_50_words)
```

**These words are more general words used to construct sentences but this was expected being that these are Tweets and not content from experts on the subject. I will use the sentiment lexicon "Loughran" to analyze the text in these tweets and see which sentiments occur most often**

```{r}
get_sentiments("loughran")
loughran_lexicon <- get_sentiments("loughran")

sentiment_analysis <- tweet_words_df %>% inner_join(get_sentiments("loughran"), by = c("word")) %>%
count(sentiment, sort = TRUE)

print(sentiment_analysis)
```

**I will now graph and show which sentiments most commonly occurred when it comes to the words in these tweets regarding Chatgpt. The most commonly occuring sentiments are positive, followed by negative and then uncertainty.**

```{r}
ggplot(sentiment_analysis, aes(x = sentiment, y = n)) +
geom_bar(stat = "identity", fill = "skyblue", color = "black") +
labs(title = "Sentiment Analysis of Tweets", x = "Sentiment", 
y = "Frequency") + theme_minimal()
```

## Refrences:

**Robinson, J. S. and D. (n.d.). 2 sentiment analysis with Tidy Data: Text mining with R. A Tidy Approach. <https://www.tidytextmining.com/sentiment>**

**SA, C. (2023) CHATGPT sentiment analysis, Kaggle.**
Available at:<https://www.kaggle.com/datasets/charunisa/chatgpt-sentiment-analysis/data> (Accessed: 28 March 2024).

**Lexicon Link** 
**Flynn, L. (2023) Comparing sentiment analysis dictionaries in R, Medium.** 
Available at:(https://medium.com/@laurenflynn1211/comparing-sentiment-analysis-dictionaries-in-r-c695fca64326){.uri} (Accessed: 28 March 2024).
